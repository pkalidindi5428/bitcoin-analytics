devtools::install_github("jrowen/twitteR", ref = "oauth_httr_1_0")
consumer_key <- "4k6po72S9kb6uXZST70JGp1Su"
consumer_secret <- "cnFUs9JqXuKg48KKY6yD9qDQLL9aD4jRujbsO82q7dBUVYekJw"
access_token <- "1337968898-I548V0LNr0nELzNBY8RtND31wn3SwLOH5j7tE2A"
access_secret <- "TTpbYJPtzGZ2z0Xj7zeeQeqzeKVSbaMoolmBregjP9N9S"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
setup_twitter_oauth()
library('twitteR')
twitteR:::setup_twitter_oauth()
library(twitteR)
consumer_key <- "4k6po72S9kb6uXZST70JGp1Su"
consumer_secret <- "cnFUs9JqXuKg48KKY6yD9qDQLL9aD4jRujbsO82q7dBUVYekJw
"
access_token <- "1337968898-I548V0LNr0nELzNBY8RtND31wn3SwLOH5j7tE2A"
access_secret <- "TTpbYJPtzGZ2z0Xj7zeeQeqzeKVSbaMoolmBregjP9N9S
"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
library(twitteR)
consumer_key <- "ywFsJbhWTbRD3X8LETfEofI04"
consumer_secret <- "cEB0iAxu86HePswlLa2t9CbGGML3gZG1BdcDUysRNV1s7NYjHm"
access_token <- "1337968898-KRw75i8x02Hk3MxSy6pNQc9nKpIQAS3bKlDLPil"
access_secret <- "i9y6rcjPl3dm1BggfbPKmxRB7ZIgGGqL9CxQB9nTWpZPT"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tw = twitteR::searchTwitter('#realDonaldTrump + #HillaryClinton', n = 1e4, since = '2016-11-08', retryOnRateLimit = 1e3)
d = twitteR::twListToDF(tw)
brexit_tweets=Get("https://api.twitter.com/1.1/search/tweets.json?q=brexit", sig)
tw = twitteR::searchTwitter('#realDonaldTrump + #HillaryClinton', n = 110, since = '2016-11-08', retryOnRateLimit = 10)
install.packages("rjson")
install.packages("rjson")
install.packages("gridExtra")
install.packages("lubridate")
library(twitteR)
libary(ggplot2)
library(ggplot2)
library(rjson)
library(gridExtra)
library(lubridate)
clinton_tweets <- userTimeline(user = "@HillaryClinton",n = 200, includeRts = FALSE, retryOnRateLimit = 2000)
clinton_tweets <- twListToDF(clinton_tweets)
textScrubber <- function(dataframe) {
dataframe$text <-  gsub("â€”", " ", dataframe$text)
dataframe$text <-  gsub("&", " ", dataframe$text)
dataframe$text <-  gsub("[[:punct:]]", " ", dataframe$text)
dataframe$text <-  gsub("[[:digit:]]", "", dataframe$text)
dataframe$text <-  gsub("http\\w+", "", dataframe$text)
dataframe$text <-  gsub("\n", " ", dataframe$text)
dataframe$text <-  gsub("[ \t]{2,}", "", dataframe$text)
dataframe$text <-  gsub("^\\s+|\\s+$", "", dataframe$text)
dataframe$text <-  tolower(dataframe$text)
return(dataframe)
}
clinton_tweets <- textScrubber(clinton_tweets)
tdmCreator <- function(dataframe, stemDoc = T, rmStopwords = T){
tdm <- Corpus(VectorSource(dataframe$text))
if (isTRUE(rmStopwords)) {
tdm <- tm_map(tdm, removeWords, stopwords())
}
if (isTRUE(stemDoc)) {
tdm <- tm_map(tdm, stemDocument)
}
tdm <- TermDocumentMatrix(tdm,
control = list(wordLengths = c(1, Inf)))
tdm <- rowSums(as.matrix(tdm))
tdm <- sort(tdm, decreasing = T)
df <- data.frame(term = names(tdm), freq = tdm)
return(df)
}
clinton_tweets <- tdmCreator(clinton_tweets)
library(lubridate)
library(ggplot2)
library(dplyr)
library(readr)
Cred$handshake(cainfo = system.file('CurlSSL', 'cacert.pem', package = 'RCurl'))
save(Cred, file='twitter authentication.Rdata')
load('twitter authentication.Rdata')
brexit_tweets<-searchTwitter("brexit",n=3200)
install.packages("twittER")
install.packages("twitteR")
library(twitteR)
library(roauth)
searchTwitter("brexit",n=3200)
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
install_github("twitteR", username="geoffjentry")
library(twitteR)
library(geoffjentry/twitteR)
install_github("twitteR", username="geoffjentry/twitteR")
library(devtools)
install_github("twitteR", username="geoffjentry")
api_key <- "DtWsEF2bTpgkxXxP64QPS5inQ"
api_secret <- "oh3gvgaoZjszb02lvNvVjbrEZtyyi67NrmFRlMzur0GkGD2EXs"
access_token <- "1337968898-WNwi5YgGxZt5c16J9maWRPDwgHQWm04jlLJfw2b"
access_token_secret <- "tZLkDlolhtF7qVPE41vqKZ4iUeCoKTVIlYI08KPYZhO4n"
setup_twitter_oauth(api_key,api_secret)
library(twitteR)
searchTwitteR("brexit",n=20)
searchTwitter("brexit")
save.image("C:/Users/lyang/Desktop/Convergent/Project.RData")
library(twitteR)
searchTwitteR("brexit")
tweets<-userTimeline("theresa_may", n=3200)
n.tweet<-length(tweets)
(n.tweet<-length(tweets))
tweets.df<-twListToDF(tweets.df)
tweets<-userTimeline("PhilipHammondUK", n=3200)
(n.tweet<-length(tweets))
tweets<-searchTwitter("philip hammond", n=3200)
searchtwitter("philip hammond")
searchTwitteR("philip hammond")
searchTwitter("teresa may")
searchTwitteR("sterling bullish")
searchTwitter("sterling ")
searchTwitter("bank of england")
brexit_tweets<-searchTwitter("brexit",n=3200)
(n.tweet<-length(tweets))
brexit_tweets.df<-twListToDF(brexit_tweets)
brexit_tweets.df[190, c("id", "created", "screenName", "replyToSN",
"favoriteCount", "retweetCount", "longitude", "latitude", "text")]
writeLines(strwrap(tweets.df$text[190], 60))
library(tm)
install.packages("NLP")
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(tweets.df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy for stem completion later
myCorpusCopy <- myCorpus
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(brexit_tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument) # stem words
writeLines(strwrap(myCorpus[[190]]$content, 60))
brexit_tweets<-searchTwitter("brexit",n=500)
library(twitteR)
searchTwitter("bank of england")
library(twitteR)
library(twitteR)
library("twitteR")
load("C:/Users/lyang/Desktop/Convergent/Project.RData")
library("twitteR")
library(twitteR)
install.packages("twitteR")
library(twitteR)
brexit_tweets<-searchTwitter("brexit",n= 3200)
load("C:/Users/lyang/Desktop/Convergent/Project.RData")
searchTwitter("brexit")
api_key <- "DtWsEF2bTpgkxXxP64QPS5inQ"api_secret <- "oh3gvgaoZjszb02lvNvVjbrEZtyyi67NrmFRlMzur0GkGD2EXs"
>
> access_token <- "1337968898-WNwi5YgGxZt5c16J9maWRPDwgHQWm04jlLJfw2b"
>
> access_token_secret <- "tZLkDlolhtF7qVPE41vqKZ4iUeCoKTVIlYI08KPYZhO4n"
>
> setup_twitter_oauth(api_key,api_secret)
api_key <- "DtWsEF2bTpgkxXxP64QPS5inQ"
api_secret <- "oh3gvgaoZjszb02lvNvVjbrEZtyyi67NrmFRlMzur0GkGD2EXs"
access_token <- "1337968898-WNwi5YgGxZt5c16J9maWRPDwgHQWm04jlLJfw2b"
access_token_secret <- "tZLkDlolhtF7qVPE41vqKZ4iUeCoKTVIlYI08KPYZhO4n"
setup_twitter_oauth(api_key,api_secret)
brexit_tweets<-searchTwitter("brexit",n=3200)
tweets.df<-twListToDF(brexit_tweets)
tweets.df[190, c("id", "created", "screenName", "replyToSN","favoriteCount", "retweetCount", "longitude", "latitude", "text")]
library(tm)
myCorpus <- Corpus(VectorSource(brexit_tweets.df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus, stemDocument) # stem words
writeLines(strwrap(myCorpus[[190]]$content, 60))
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
writeLines(strwrap(myCorpus[[190]]$content, 60))
stemCompletion2 <- function(x, dictionary) {
x <- unlist(strsplit(as.character(x), " "))
x <- x[x != ""]
x <- stemCompletion(x, dictionary=dictionary)
x <- paste(x, sep="", collapse=" ")
PlainTextDocument(stripWhitespace(x))
}
myCorpus <- lapply(myCorpus, stemCompletion2, dictionary=myCorpusCopy)
myCorpus <- Corpus(VectorSource(myCorpus))
writeLines(strwrap(myCorpus[[190]]$content, 60))
wordFreq <- function(corpus, word) {
results <- lapply(corpus,
function(x) { grep(as.character(x), pattern=paste0("\\<",word)) }
)
sum(unlist(results))
}
n.sterling<-wordFreq(myCorpusCopy, "sterling")
n.brexit<-wordFreq(myCorpusCopy,"brexit")
n.teresamay<-wordFreq(myCorpusCopy,"teresa may")
cat(n.sterling,n.brexit)
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub),
pattern=oldword, replacement=newword)
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub),
pattern=oldword, replacement=newword)
}
myCorpus <- replaceWord(myCorpus, "Majesti", "Majesty")
freq.terms<-findFreqTerms((tdm,lowfreq=20))
(freq.terms <- findFreqTerms(tdm, lowfreq = 20))
myCorpus <- replaceWord(myCorpus, "peopl", "people")
myCorpus <- replaceWord(myCorpus, "opposit", "oppose")
tdm <- TermDocumentMatrix(myCorpus,
control = list(wordLengths = c(1, Inf)))
tdm
myCorpus <- replaceWord(myCorpus, "Govern", "Government")
tdm <- TermDocumentMatrix(myCorpus,
control = list(wordLengths = c(1, Inf)))
tdm
idx <- which(dimnames(tdm)$Terms %in% c("government", "majesty", "people"))
as.matrix(tdm[idx, 21:30])
(freq.terms <- findFreqTerms(tdm, lowfreq = 20))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 20)
df <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip() +
theme(axis.text=element_text(size=7))
library(NLP)
library(ggplot2)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip() +
theme(axis.text=element_text(size=7))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
library(wordcloud)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,random.order = F, colors = pal)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,random.order = F)
findAssocs(tdm, "r", 0.2)
findAssocs(tdm, "sterling", 0.2)
findAssocs(tdm, "brexit", 0.2)
library(graph)
install.packages("graph")
library(graph)
library(Rgraphviz)
install.packages(Rgraphviz)
dtm <- as.DocumentTermMatrix(tdm)
install.packages("topicmodels")
library(topicmodels)
lda<-LDA(dtm, k = 8)
Topic 3
## "data, science, group, poll, kdnuggets, package, software"
## Topic 4
## "r, data, talk, slide, mining, analysing, dataset"
## Topic
term <- terms(lda, 7)
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))
topics <- topics(lda)
topics <- data.frame(date=as.IDate(tweets.df$created), topic=topics)
ggplot(topics, aes(date, fill = term[topic])) +
geom_density(position = "stack")
require(devtools)
install_github("sentiment140", "okugami79")
require(devtools)
install_github("sentiment140", "okugami79")
install.packages("Rsentiment")
install.packages('Rsentiment')
install.packages("sentiment")
install.packages("sentimentanalysis")
install.packages("syuzhet")
library(syuzhet)
brexit_sentiment<-get_sentiment(brexit_tweets,method="syuzhet")
brexit_sentiment<-get_sentiment(tdm,method="syuzhet")
View(tdm)
require(devtools)
install_github("sentiment140", "okugami79")
install_github(okugami79/sentiment140)
install_github("okugami79/sentiment140")
install.packages("sentiment")
library(sentiment)
sentiment("brexit sucks")
brexit_tweets
sentiment(brexit_tweets)
View(brexit_tweets)
sentiment(brexit_tweets$text)
brexit_tweets["text"]
brexit_tweets$text
brexit_tweets$$text
c(brexit_tweets$text)
c(brexit_tweets->text)
c(brexit_tweets->text)
sentiment(c(brexit_tweets->text))
brexit_tweets[0]
brexit_tweets[1]
brexit_tweets[1]->text
brexit_tweets[1]->text
brexit_tweets[15]->text
brexit_tweets[15]
sentiment(brexit_tweets->text)
brexit_tweets->text
c(brexit_tweets[1]->text)
c(brexit_tweets[15]->text)
View(brexit_tweets.df)
c(brexit_tweets[15]->text)
brexit_tweets.df[0]
c(brexit_tweets.df[0])
c(brexit_tweets.df[[0]])
c(brexit_tweets.df[['text']])
brexit_vector<-c(brexit_tweets.df[['text']])
sentiment(brexit_vector)
sentiment(brexit_vector[0:10])
brexit_vector
brexit_vector[0:10]
sentiment(brexit_vector[0:10])
sentiment(brexit_vector[0:10])
sentiment(brexit_vector[0:100])
save.image("C:/Users/lyang/Desktop/Convergent/project1.RData")
sentiment(brexit_vector[0:100])
sentiment(brexit_vector[0:10])
sum(brexit_vector)
brexit_vector2<-get_sentiment(brexit_vector,method="syuzhet")
trial_vector<-get_sentiment(brexit_tweets,method="syuzhet")
head(brexit_vector)
brexit_vector_copy<-brexit_vector
rbind(sign(head(brexit_vector_copy)))
brexit_token<-get_tokens(brexit_vector,pattern = "\\W")
Numerical_sentiment<-get_sentiment(brexit_token,method="syuzhet")
get_sentiments(brexit_tweets)
get_sentiment(brexit_vector)
get_sentiment(brexit_tweets.df)
brexit_tweets$text <- sapply(brexit_tweets$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
get_sentiment(brexit_vector)
library(twitteR)
library(tm)
library(wordcloud   )
library(RColorBrewer)
library(ggplot2)
myCorpusCopy<-Corpus(VectorSource(brexit_tweets))
tdmcopy<-TermDocumentMatrix(myCorpus,control = list(removePunctuation = TRUE,stopwords = c("brexit", stopwords("english")), removeNumbers = TRUE, tolower = TRUE))
m<-as.matrix((tdmcopy))
word_frequency<-sort(rowsum(m),decreasing=true())
word_freq<-sort(rowSums(m),decreasing=true)
word_freq<-sort(rowSums(m),decreasing= partial)
sentiment(brexit_vector[0:10])
count(sentiment,sort=true)
sentiment(brexit_vector[0:10])
library(sentiment)
sentiment(brexit_vector[0:10])
sentiment(brexit_vector[0:10])
savehistory("C:/Users/lyang/Desktop/Convergent/brexit_analysis.Rhistory")
